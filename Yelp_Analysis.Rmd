---
title: "Yelp Data Analysis"
output: html_document
---

In this analysis, my goal is to see how various non-food factors affect yelp reviews. 

We'll start by looking at restaurant ratings on yelp. Then, we'll see how specific attributes of restaurants affect the ratings. Finally, I'll make a predictive model using these attributes as inputs. We'll train the model with the first half of the data set, and test it using the second half of the data set to see how accurate the model is.

```{r, echo=FALSE, message=FALSE}
library(rjson)
library(plyr)
library(dplyr)
library(ggplot2)
library(knitr)
library(glmnet)
```

```{r, echo=FALSE}
# --------------------------------------------------------------------------------------------------------
# The following makes the .json file a .rdata file. Once the .rdata file has been created, we don't need
# the code anymore.
# --------------------------------------------------------------------------------------------------------

# Open file, read all the lines, then close the file.
# con <- file("yelp_academic_dataset_business.json", "r")
# input <- readLines(con, -1L)
# close(con)

# Extract each json object (i.e. a single buisiness) from the read file,
# and make it a row of a data frame.
# yelpdata <- input %>%
#   lapply(function(x) t(unlist(fromJSON(x)))) %>% 
#   ldply()
# save(yelpdata, file= 'yelpdata.rdata')
```

```{r, echo=FALSE}
# Load the data.
load("yelpdata.rdata")

#Clean data.
clean.names <- function(df){
  colnames(df) <- gsub("[^[:alnum:]]", "", colnames(df))
  colnames(df) <- tolower(colnames(df))
  return(df)
}
yelpdata <- clean.names(yelpdata)
yelpdata <- yelpdata[,!duplicated(colnames(yelpdata))]

# Modify data. 
yelpdata$stars <- as.numeric(as.character(yelpdata$stars))
yelpdata$reviewcount <- as.numeric(as.character(yelpdata$reviewcount))
names(yelpdata)[names(yelpdata)=="attributeshappyhour"] <- "happyhour"
names(yelpdata)[names(yelpdata)=="attributesacceptscreditcards"] <- "acc"
names(yelpdata)[names(yelpdata)=="attributesgoodforgroups"] <- "groups"
names(yelpdata)[names(yelpdata)=="attributesoutdoorseating"] <- "outdoor"
names(yelpdata)[names(yelpdata)=="attributespricerange"] <- "price"
names(yelpdata)[names(yelpdata)=="attributesalcohol"] <- "alcohol"
names(yelpdata)[names(yelpdata)=="attributesnoiselevel"] <- "noiselevel"
names(yelpdata)[names(yelpdata)=="attributesambienceclassy"] <- "classy"
names(yelpdata)[names(yelpdata)=="attributesparkingvalet"] <- "valet"
names(yelpdata)[names(yelpdata)=="neighborhoods"] <- "nhood"
names(yelpdata)[names(yelpdata)=="attributesdrivethru"] <- "drivethru"
names(yelpdata)[names(yelpdata)=="attributesparkinglot"] <- "parkinglot"
names(yelpdata)[names(yelpdata)=="attributesparkinglot"] <- "parkinglot"
names(yelpdata)[names(yelpdata)=="attributespaymenttypescashonly"] <- "cash"
names(yelpdata)[names(yelpdata)=="attributesambiencecasual"] <- "casual"
names(yelpdata)[names(yelpdata)=="attributesgoodfordancing"] <- "dance"
names(yelpdata)[names(yelpdata)=="attributesdelivery"] <- "delivery"
names(yelpdata)[names(yelpdata)=="attributescoatcheck"] <- "ccheck"
names(yelpdata)[names(yelpdata)=="attributestakeout"] <- "takeout"
names(yelpdata)[names(yelpdata)=="attributestakesreservations"] <- "res"
names(yelpdata)[names(yelpdata)=="attributeswaiterservice"] <- "service"
names(yelpdata)[names(yelpdata)=="attributesparkingstreet"] <- "street"
names(yelpdata)[names(yelpdata)=="attributesparkinggarage"] <- "garage"
names(yelpdata)[names(yelpdata)=="attributesgoodforlatenight"] <- "late"
names(yelpdata)[names(yelpdata)=="attributesgoodfordessert"] <- "desert"
names(yelpdata)[names(yelpdata)=="attributescaters"] <- "caters"
names(yelpdata)[names(yelpdata)=="attributeswifi"] <- "wifi"
names(yelpdata)[names(yelpdata)=="attributesattire"] <- "attire"
names(yelpdata)[names(yelpdata)=="attributeshastv "] <- "tv"



# Change <NA> to "dnr" (did not respond).
addDNR <- function(x){
  if(is.factor(x)) return(factor(x, levels=c(levels(x), "dnr")))
  return(x)
}
yelpdata <- as.data.frame(lapply(yelpdata, addDNR))
yelpdata[is.na(yelpdata)] <- "dnr"

# Make city/state column to consolidate and clarify vague city and state labels.
yelpdata <- mutate(yelpdata, loc = ifelse(yelpdata$state=="NV", "Las Vegas, NV",
                                        ifelse(yelpdata$state=="PA", "Pittsburg, PA",
                                          ifelse(yelpdata$state=="NC", "Charlotte, NC",
                                            ifelse(yelpdata$state=="AZ", "Phoenix, AZ",
                                              ifelse(yelpdata$state=="IL", "Urbana-Champaign, IL",
                                                ifelse(yelpdata$state=="WI", "Madison, WI",
                                                  ifelse(yelpdata$state=="MLN", "Edinburgh, UK",
                                                    ifelse(yelpdata$state=="BW", "Karlsruhe, Germany",
                                                      ifelse(yelpdata$state=="QC", "Montreal, Canada",  
                                                       ifelse(yelpdata$state=="ON", "Waterloo, Canada",
                                                        ifelse(yelpdata$state=="SC", "Fort Mill, SC",
                                                         ifelse(yelpdata$state=="EDH", "Edinburgh, UK",
                                                          ifelse(yelpdata$state=="KHL", "Edinburgh, UK",
                                                           ifelse(yelpdata$state=="XGL", "Edinburgh, UK",
                                                            ifelse(yelpdata$state=="NTH", "Edinburgh, UK",
                                                            ifelse(yelpdata$state=="SCB", "Edinburgh, UK",
                                                         NA)))))))))))))))))

chinese <- filter(yelpdata, categories1 == "Chinese" & categories2 == "Restaurants") 
all_restaurants <- filter(yelpdata, categories == "Restaurants" |
                     categories1 == "Restaurants" | 
                     categories2 == "Restaurants"| 
                     categories3 == "Restaurants"|
                     categories4 == "Restaurants"|
                     categories5 == "Restaurants"|
                     categories6 == "Restaurants"|
                     categories7 == "Restaurants") 
```

## Data Exploration - Restaurant Star Ratings
 
Let's look at the overall number of restaurants by rating. Naively we'd expect a normal distribution, but the plot shows a right skew. We can expect that (1) people like going to and reviewing good restaurants, (2) as a result better restaurants tend to stick around and the bad ones tend to close, and (3) people may not be rating "normal"-y.

```{r, echo=FALSE}
ggplot(all_restaurants, aes(as.factor(stars))) + 
  geom_histogram() +
  xlab("Stars") +
  ylab("Number of Restaurants")
```

What do we expect to see in a plot comparing the number of restaurants with the number of reviews each restaurant recieved?

If (1) were true, we'd expect to have a large number of restuarants with high ratings
that have been around for a while.

```{r, echo=FALSE}
ggplot(all_restaurants, aes(reviewcount)) + 
  geom_histogram(binwidth = .15) + 
  scale_x_log10() +
  xlab("Number of Reviews") +
  ylab("Number of Restaurants")
```

Also, this plot seems to support (1) and (2), since there seem to be quite a few restaurants with low reviews.

```{r,echo = FALSE}
ggplot(all_restaurants, aes(reviewcount)) + 
  geom_histogram(binwidth = .15) + 
  scale_x_log10() +
  facet_wrap(~ stars) +
  xlab("Number of Reviews") +
  ylab("Number of Restaurants")
```

For the most part, the left skewed distribution seems relatively consistent for all ratings. As we go from 1 star restaurants to 4 star restaurants though, we see that the peak of the skewed distribution keeps moving to the right, indicating that there are more reviews for better rated restaurants. However, for restaurants of 4.5 and 5 stars, the peak scoots back to the left. We can get a sense of this by looking at a boxplot of the number of reviews by star rating.


```{r, echo=FALSE}
ggplot(all_restaurants, aes(as.factor(stars), reviewcount)) + 
  geom_boxplot() +
  scale_y_log10() +
  xlab("Stars") +
  ylab("Number of Reviews")

#dt <- all_restaurants %>% 
#  group_by(stars) %>%
#  summarise(avg_num_rev = round(mean(review_count), digits =  2)) %>%
#  tbl_df()

#kable(dt, align  = "c")
```

What can we make of the low mean number of reviews for 5 star restaurants? Perhaps that most 5 star restaurants are recieve that rating by chance; with a small sample size of 3 or 4 reviews, there will inevitably be a few restaurants that are rated 5 stars for each of those reviews. As the sample size (number of reviews) increases, the yelp rating will begin to approach this restaurants "true" rating, which will most likely not be 5 stars. The same probably occurs for 1 star restaurants as well, but there is an added consequence: if you are a 1 star restaurant, you probably won't be open for very long...

Finally, let's just see what cities have the best ratings. 

```{r, echo=FALSE}
dstate <- all_restaurants %>%
  group_by(loc) %>%
  summarise(avg_stars = round(mean(stars), digits =  2), 
            avg_num_rev = round(mean(reviewcount), digits =  2),
            num_rest = n()) %>%
  tbl_df()

dstate <- dstate[order(-dstate$avg_stars),]

kable(dstate, align  = "c")
```

## LASSO Model

Now we want to create a model that uses various bussiness attributes to predict the star rating of a restaurant. 

The LASSO flavor of linear regession involves finding an ideal parameter in order to reduce possibility of overfitting. When fitting a linear regression, we are looking for the values $b_j$ that correspond to inputs $x_j$ and $\hat{y}$:

$$ \hat{y}~=~b_0~+~b_1x_1~+~...~+~b_kx_k . $$

Normally in linear regression, we seek to find the coefficients that minimize the value of $\sum{(y-\hat{y})^2}$, but with LASSO regression we impose an additional constraint: 

$$\sum{| b_j |} \leq s,$$

so the sum of the magnitude of all of the coefficients cannot exceed the value of $s$, a tunable parameter. If we make $s$ small (but still greater than zero), then the coefficients of unimportant parameters go to zero, and are thus not really included in the model. As a result, only the variables with a significant impact will appear in the model.

How do we find this value of $s$? It's a trade-off between the smallest error and the fewest number of variables. 

As predictors, we'll most of the relevant variables included for \textit{restaurants} in the dataset. This includes categories (i.e. Chinese, American, Breakfast), the geographic location (city, neighborhood), attributes (parking, wifi). 

We'll examine the accuracy of this model by splitting the data set in half. The first half will be used to train the model that will be tested on the second half. 

```{r, echo=FALSE}
# Make dataset with predictors.
dataset <- all_restaurants %>%
  select(stars,
         categories1, 
         categories2,
         categories3,
         categories4,
         categories5,
         categories6,
         categories7,
         city,
         price,
         alcohol,
         noiselevel,
         classy,
         valet,
         cash,
         nhood,
         drivethru,
         parkinglot,
         casual,
         dance,
         delivery,
         ccheck,
         takeout,
         res,
         service,
         street,
         garage,
         late,
         desert,
         caters,
         wifi)

# Define make predictors into a matrix.
x <- model.matrix(stars ~ ., data = dataset)[,-1]
y <- dataset$stars

# Define training and test sets.
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.train <- y[train]
y.test <- y[! (1:nrow(x)) %in% train]
grid=10^seq(10,-2, length =100)

# Train the LASSO model. Make plot of coefficients for increasing "s".
lasso.mod <- glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)
```

The plot above shows how the values of coefficients change for an increasing lasso parameter $s$ (x-axis).
The key variables are the ones that ``jump out" early on. 

Below we see the change in Mean-Squared Error ($\frac{1}{k}\sum{(y-\hat{y})^2}$) as a function of a changing $s$ (which is proportional to the $log(\lambda)$ at the bottom of the x-axis). The top of the x-axis shows the number of variables included in the model. We find the ideal $s$ value at the dip, where the mean-square value is the lowest.

```{r, echo=FALSE}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min


lasso <- data.frame(y.act = y.test)
lasso$y.act <- lasso$y.act %>%
  as.character() %>%
  as.numeric()
lasso$pred <- predict(lasso.mod, s=bestlam, newx=x[test,])  %>%
  as.character() %>%
  as.numeric()
# mean((lasso$pred -y.test)^2)

# Plot of actual vs. predicted values for the test data set.
ggplot(lasso, aes(jitter(y.act), pred)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  geom_abline(intercept = 0, colour = "red", size = 2) +
  coord_cartesian(ylim = c(1,5)) +
  xlab("Actual Star Ratings") +
  ylab("Predicted Star Ratings")
```

The plot above compares the actual and the predicted results. The red line shows where a perfect predictions would fall, and the black points represent individual data points. Our model does a good job of capturing the overall trend of the star ratings, but fails to predict any outliers. Most predictions are between 4.5 and 2.5 stars, but fails to predict any outliers of 1, 2, or even 5 stars. But this sort of makes sense because most restaurants have ratings of 3.5 or 4 stars, as we saw at the very beginning. 

The largest coefficients are shown below:

```{r, echo=FALSE}
out <- glmnet(x, y, alpha=1, lambda=grid)
lasso.coef <- predict(out, type="coefficients", s=bestlam) [1:300,]
tail(sort(lasso.coef),5)
head(sort(lasso.coef),5)
```

